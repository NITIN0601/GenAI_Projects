# Financial RAG System ğŸš€

A production-ready **Retrieval Augmented Generation (RAG)** system for analyzing financial PDFs using **100% FREE local models**. No API keys required!

## âœ¨ Features

- ğŸ†“ **Completely Free** - Uses local models (Ollama + sentence-transformers)
- ğŸ“„ **Advanced PDF Scraping** - Handles 2-column layouts, extracts tables with metadata
- ğŸ§  **Semantic Search** - ChromaDB vector database for intelligent retrieval
- âš¡ **Redis Caching** - Fast responses with intelligent caching
- ğŸ·ï¸ **Rich Metadata** - SourceDoc, ChunkReferenceID, PageNo, Table_Title, Year, Quarter, and more
- ğŸ’¬ **Interactive CLI** - Beautiful command-line interface
- ğŸ¯ **Smart Filtering** - Query by year, quarter, table type

## ğŸ› ï¸ Tech Stack

| Component | Technology | Why? |
|-----------|-----------|------|
| **LLM** | Ollama (llama3.2) | Free, local, no API keys |
| **Embeddings** | sentence-transformers | Free, local, fast |
| **Vector DB** | ChromaDB | Lightweight, embedded |
| **Cache** | Redis | Performance optimization |
| **Framework** | LangChain | Best for RAG pipelines |
| **PDF Parsing** | pdfplumber | Handles complex layouts |

## ğŸ“‹ Prerequisites

### 1. Install Ollama (Free LLM)

```bash
# macOS
brew install ollama

# Start Ollama
ollama serve

# Pull the model (in a new terminal)
ollama pull llama3.2
```

### 2. Install Redis (Optional but recommended)

```bash
# macOS
brew install redis
brew services start redis
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd GENAI
pip install -r requirements.txt
```

### 2. Check System Setup

```bash
python main.py setup
```

This will verify:
- âœ“ Ollama is running
- âœ“ Redis is available
- âœ“ Embedding model is loaded
- âœ“ Vector store is ready

### 3. Index Your PDFs

```bash
# Index all PDFs from raw_data directory
python main.py index --source ../raw_data

# Clear existing index and rebuild
python main.py index --source ../raw_data --clear
```

**Expected output:**
```
ğŸ“š Indexing Financial PDFs

Found 6 PDF files

âœ“ 10k1224.pdf: 45 tables, 892 chunks (12.34s)
âœ“ 10q0320.pdf: 38 tables, 756 chunks (10.21s)
...

Indexing Summary
âœ“ Successfully processed: 6/6 files
âœ“ Total tables indexed: 234
âœ“ Total chunks created: 4,521
```

### 4. Query the System

```bash
# Simple query
python main.py query "What was the total revenue in Q2 2025?"

# Query with filters
python main.py query "Show balance sheet items" --year 2024 --quarter Q3

# Interactive mode
python main.py interactive
```

## ğŸ“– Usage Examples

### Command Line Queries

```bash
# General question
python main.py query "What are the total assets?"

# Filtered by year
python main.py query "Show revenue breakdown" --year 2025

# Filtered by quarter
python main.py query "Cash flow analysis" --quarter Q2

# Retrieve more context
python main.py query "Compare balance sheets" --top-k 10

# Disable cache
python main.py query "Latest earnings" --no-cache
```

### Interactive Mode

```bash
python main.py interactive
```

Then ask questions naturally:
```
Your question: What was the total revenue in Q2 2025?
Your question: Show me all balance sheet items for 2024
Your question: Compare cash flow across quarters
```

### System Statistics

```bash
python main.py stats
```

Shows:
- Total chunks indexed
- Number of documents
- Years covered
- Cache statistics

### Clear Cache

```bash
python main.py clear-cache
```

### Rebuild Index

```bash
python main.py rebuild-index
```

## ğŸ“Š Metadata Schema

Each table chunk includes rich metadata:

```python
{
    "source_doc": "10q0625.pdf",           # Source PDF
    "chunk_reference_id": "uuid-here",     # Unique ID
    "page_no": 5,                          # Page number
    "table_title": "Consolidated Balance Sheet",
    "year": 2025,                          # Fiscal year
    "quarter": "Q2",                       # Quarter (for 10-Q)
    "report_type": "10-Q",                 # 10-K or 10-Q
    "table_type": "Balance Sheet",         # Classified type
    "fiscal_period": "June 30, 2025"       # Extracted period
}
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚
â”‚   Query     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Query Engine (RAG)          â”‚
â”‚  1. Parse query                 â”‚
â”‚  2. Extract filters             â”‚
â”‚  3. Check cache                 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Retriever                   â”‚
â”‚  - Semantic search              â”‚
â”‚  - Metadata filtering           â”‚
â”‚  - Context building             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vector Store (ChromaDB)       â”‚
â”‚  - 4,521 chunks                 â”‚
â”‚  - Embeddings (384-dim)         â”‚
â”‚  - Metadata filters             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM (Ollama)                  â”‚
â”‚  - Generate answer              â”‚
â”‚  - Include citations            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Response   â”‚
â”‚  + Sources  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
GENAI/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py          # Configuration
â”‚   â””â”€â”€ prompts.py           # LLM prompts
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ pdf_scraper.py       # Enhanced PDF scraping
â”‚   â””â”€â”€ metadata_extractor.py # Metadata extraction
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ embedding_manager.py # Embedding generation
â”‚   â””â”€â”€ vector_store.py      # ChromaDB operations
â”œâ”€â”€ cache/
â”‚   â””â”€â”€ redis_cache.py       # Redis caching
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ retriever.py         # Retrieval logic
â”‚   â”œâ”€â”€ llm_manager.py       # LLM integration
â”‚   â””â”€â”€ query_engine.py      # Main RAG pipeline
â”œâ”€â”€ models/
â”‚   â””â”€â”€ schemas.py           # Pydantic models
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ helpers.py           # Utility functions
â”œâ”€â”€ main.py                  # CLI application
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.md               # This file
```

## âš™ï¸ Configuration

Edit `config/settings.py` or create a `.env` file:

```env
# LLM Settings
LLM_MODEL=llama3.2
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2000

# Embedding Settings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Redis Settings
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_ENABLED=true

# Retrieval Settings
TOP_K=5
SIMILARITY_THRESHOLD=0.7
```

## ğŸ¯ Advanced Features

### Custom Filters

```python
from rag import get_query_engine

engine = get_query_engine()
response = engine.query(
    query="Show revenue",
    filters={
        "year": 2025,
        "quarter": "Q2",
        "table_type": "Income Statement"
    }
)
```

### Programmatic Usage

```python
from scrapers import EnhancedPDFScraper, MetadataExtractor
from embeddings import get_vector_store, get_embedding_manager

# Extract tables
scraper = EnhancedPDFScraper("path/to/file.pdf")
tables = scraper.extract_all_tables()

# Create chunks and index
# ... (see main.py for full example)
```

## ğŸ› Troubleshooting

### Ollama not running

```bash
# Start Ollama
ollama serve

# In another terminal, pull the model
ollama pull llama3.2
```

### Redis connection failed

```bash
# Start Redis
brew services start redis

# Or disable Redis in settings
REDIS_ENABLED=false
```

### Embedding model download slow

The first run will download the sentence-transformers model (~90MB). This is a one-time download.

### Out of memory

Reduce batch size in `config/settings.py`:
```python
EMBEDDING_BATCH_SIZE=16  # Default is 32
```

## ğŸ“ˆ Performance

- **Indexing**: ~2-3 seconds per PDF
- **Query**: ~2-5 seconds (first query), <1 second (cached)
- **Embedding**: ~100 chunks/second
- **Memory**: ~500MB (embedding model) + ~200MB (vector DB)

## ğŸ”’ Privacy

- âœ… All data stays on your machine
- âœ… No external API calls
- âœ… No data sent to cloud services
- âœ… Complete control over your data

## ğŸ¤ Contributing

This is a production-ready system. Feel free to extend it with:
- Additional LLM models
- More sophisticated chunking strategies
- Custom metadata extractors
- API endpoints (FastAPI)
- Web UI

## ğŸ“ License

MIT License - Use freely!

## ğŸ™ Acknowledgments

- **Ollama** - Free local LLM inference
- **sentence-transformers** - Free embeddings
- **ChromaDB** - Lightweight vector database
- **LangChain** - RAG framework
- **pdfplumber** - PDF parsing

---

**Built with â¤ï¸ using 100% free and open-source tools**
